{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fixing import errors of the\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# This code navigates up one directory from the notebook's location ('examples/')\n",
    "# to get the project's root directory.\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# We check if the path is already in the system path.\n",
    "# If not, we add it to the beginning of the list.\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added project root to Python path: {project_root}\")\n",
    "else:\n",
    "    print(f\"Project root is already in Python path: {project_root}\")\n",
    "\n",
    "# Optional: You can print the first few paths to verify\n",
    "print(\"\\nVerifying sys.path:\")\n",
    "for i, path in enumerate(sys.path[:5]):\n",
    "    print(f\"{i}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import logging\n",
    "import nest_asyncio\n",
    "import argparse\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Apply nest_asyncio for Jupyter environments\n",
    "nest_asyncio.apply()\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Document\n",
    "from src.GlirelPathExtractor import GlirelPathExtractor \n",
    "from src.RecursivePathExtractor import RecursiveLLMPathExtractor\n",
    "from llama_index.core import SimpleDirectoryReader, PropertyGraphIndex,Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4bf3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../.data/novel.json', 'r') as file:\n",
    "    # Load the JSON data from the file into a Python object\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522aa375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_questions_by_source(question_list):\n",
    "    grouped_questions = {}\n",
    "\n",
    "    for question in question_list:\n",
    "        source = question.get(\"source\")\n",
    "\n",
    "        if source not in grouped_questions:\n",
    "            grouped_questions[source] = []\n",
    "\n",
    "        grouped_questions[source].append(question)\n",
    "\n",
    "    return grouped_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b164c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model= \"gemma3:12b\",\n",
    "    request_timeout=120.0,\n",
    "    context_window=8128,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size=512\n",
    "Settings.chunk_overlap=64\n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"snowflake-arctic-embed2:latest\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86745897",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "---Role---\n",
    "You are a helpful assistant responding to user queries.\n",
    "\n",
    "---Goal---\n",
    "Generate direct and concise answers based strictly on the provided Knowledge Base.\n",
    "Respond in plain text without explanations or formatting.\n",
    "Maintain conversation continuity and use the same language as the query.\n",
    "If the answer is unknown, respond with \"I don't know\".\n",
    "\n",
    "---Conversation History---\n",
    "{history}\n",
    "\n",
    "---Knowledge Base---\n",
    "{context_data}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b5270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize rag\n",
    "index = load_index_from_storage(\n",
    "    StorageContext.from_defaults(persist_dir=\"./.persistent_storage/llm/Novel-30752\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.property_graph import (\n",
    "    PGRetriever,\n",
    "    VectorContextRetriever,\n",
    "    LLMSynonymRetriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e67f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_retrievers = [\n",
    "    VectorContextRetriever(index.property_graph_store),\n",
    "    LLMSynonymRetriever(index.property_graph_store),\n",
    "]\n",
    "\n",
    "retriever = index.as_retriever(path_depth=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ff917",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    include_text=\"True\",\n",
    "    response_mode=\"tree_summarize\",\n",
    "    embedding_mode=\"hybrid\",\n",
    "    similarity_top_k=5,\n",
    "    path_depth = 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ce875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c1384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine  = RetrieverQueryEngine.from_args(\n",
    "   retriever=retriever, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a183bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nest_asyncio.apply()\n",
    "response = await query_engine.aquery(\"In the context of the ancient Maya civilization as discussed in the text, which archaeological site is specifically noted for containing inscriptions created by the Mayas?\")\n",
    "print(response.response)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grag-llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
