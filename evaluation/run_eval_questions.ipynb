{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fixing import errors of the\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# This code navigates up one directory from the notebook's location ('examples/')\n",
    "# to get the project's root directory.\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# We check if the path is already in the system path.\n",
    "# If not, we add it to the beginning of the list.\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added project root to Python path: {project_root}\")\n",
    "else:\n",
    "    print(f\"Project root is already in Python path: {project_root}\")\n",
    "\n",
    "# Optional: You can print the first few paths to verify\n",
    "print(\"\\nVerifying sys.path:\")\n",
    "for i, path in enumerate(sys.path[:5]):\n",
    "    print(f\"{i}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "#llama index imports\n",
    "from llama_index.core import SimpleDirectoryReader, PropertyGraphIndex,Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "import openlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd5dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "openlit.init(\n",
    "  otlp_endpoint=\"http://127.0.0.1:4318\",\n",
    "  application_name=\"query2\",\n",
    "  environment=\"obama_enviroment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set CUDA device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Import HippoRAG components after setting environment\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(\"llamaindex_processing_test.log\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c72c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model= \"gemma3:12b\",\n",
    "    request_timeout=120.0,\n",
    "    context_window=4096, #8128,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size=512\n",
    "Settings.chunk_overlap=64\n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"snowflake-arctic-embed2:latest\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_questions_by_source(question_list: List[dict]) -> Dict[str, List[dict]]:\n",
    "    \"\"\"Group questions by their source\"\"\"\n",
    "    grouped_questions = {}\n",
    "    for question in question_list:\n",
    "        source = question.get(\"source\")\n",
    "        if source not in grouped_questions:\n",
    "            grouped_questions[source] = []\n",
    "        grouped_questions[source].append(question)\n",
    "    return grouped_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21296b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_corpus(\n",
    "    corpus_name: str,\n",
    "    questions: List[dict],\n",
    "    mode: str\n",
    "):\n",
    "    \"\"\"Process a single corpus: index it and answer its questions\"\"\"\n",
    "    logging.info(f\"üìö Processing corpus: {corpus_name}\")\n",
    "    \n",
    "    # Prepare output directory\n",
    "    output_dir = f\"./.persistent_storage/.results2/{mode}/{corpus_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"predictions_{corpus_name}.json\")\n",
    "    \n",
    "    if Path(output_path).is_file():\n",
    "        logging.info(f\"BASE: {corpus_name} has alleady been analized\")\n",
    "        return\n",
    "\n",
    "    # initialize Llama-index retrieval engine\n",
    "    strorage_cotext = StorageContext.from_defaults(persist_dir=f\"./.persistent_storage/.storage_context/{mode}/{corpus_name}\")\n",
    "\n",
    "\n",
    "    # Get questions for this corpus\n",
    "    corpus_questions = questions.get(corpus_name, [])\n",
    "    if not corpus_questions:\n",
    "        logging.warning(f\"‚ö†Ô∏è No questions found for corpus: {corpus_name}\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    logging.info(f\"üîç Found {len(corpus_questions)} questions for {corpus_name}\")\n",
    "    \n",
    "    # Prepare queries and gold answers\n",
    "    all_queries = [q[\"question\"] for q in corpus_questions]\n",
    "    gold_answers = [[q['answer']] for q in corpus_questions]\n",
    "    \n",
    "    # initlaize RAG engine\n",
    "    index = load_index_from_storage(storage_context=strorage_cotext)\n",
    "\n",
    "\n",
    "    logging.info(f\"‚úÖ Indexed corpus: {corpus_name}\")\n",
    "    query_engine = index.as_query_engine(\n",
    "        llm=llm,\n",
    "        response_mode=\"compact\",\n",
    "        similarity_top_k=8,\n",
    "        embedding_mode=\"hybrid\",\n",
    "        include_text=True, \n",
    "    )\n",
    "\n",
    "\n",
    "    # Process questions\n",
    "    results = []\n",
    "    solutions =[]\n",
    "    for query in all_queries:\n",
    "        #nest_asyncio.apply()\n",
    "        \n",
    "        try:\n",
    "            response_object = await query_engine.aquery(query)\n",
    "        except Exception as e:\n",
    "            all_errors.append(f\"{mode} : {corpus_name}\")\n",
    "            return\n",
    "        solution_dict = {\"question\":query,\n",
    "                         \"answer\":response_object.response,\n",
    "                         \"docs\":response_object.get_formatted_sources(10000)\n",
    "                         }\n",
    "        solutions.append(solution_dict)\n",
    "    for question in corpus_questions:\n",
    "        solution = next((sol for sol in solutions if sol['question'] == question['question']), None)\n",
    "        if solution:\n",
    "            results.append({\n",
    "                \"id\": question[\"id\"],\n",
    "                \"question\": question[\"question\"],\n",
    "                \"source\": corpus_name,\n",
    "                \"context\": solution.get(\"docs\", \"\"),\n",
    "                \"evidence\": question.get(\"evidence\", \"\"),\n",
    "                \"question_type\": question.get(\"question_type\", \"\"),\n",
    "                \"generated_answer\": solution.get(\"answer\", \"\"),\n",
    "                \"gold_answer\": question.get(\"answer\", \"\")\n",
    "            })\n",
    "    \n",
    "    # Save results\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    logging.info(f\"üíæ Saved {len(results)} predictions to: {output_path}\")\n",
    "    #print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"../.data/novel.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus_data = json.load(f)\n",
    "    logging.info(f\"üìñ Loaded corpus with {len(corpus_data)} documents from ../.data/novel.json\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"‚ùå Failed to load corpus: {e}\")\n",
    "    #return\n",
    "\n",
    "# Sample corpus data if requested\n",
    "\n",
    "# Load question data\n",
    "try:\n",
    "    with open(\"../.data/novel_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        question_data = json.load(f)\n",
    "    grouped_questions = group_questions_by_source(question_data)\n",
    "    logging.info(f\"‚ùì Loaded questions with {len(question_data)} entries from ../.data/novel_questions.json\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"‚ùå Failed to load questions: {e}\")\n",
    "    #return\n",
    "\n",
    "# Process each corpus in the subset\n",
    "for mode in [\"embed\",#\"gli\",\"hybrid\",\"llm\"\n",
    "             ]:\n",
    "    for item in corpus_data:\n",
    "        corpus_name = item[\"corpus_name\"]\n",
    "        context = item[\"context\"]\n",
    "        nest_asyncio.apply()\n",
    "        await process_corpus(\n",
    "            corpus_name=corpus_name,\n",
    "            questions=grouped_questions,\n",
    "            mode=mode\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d633664",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grag-llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
