{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccab7cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "def calculate_scores(directory_path: str) -> Tuple[Dict[str, Any], float]:\n",
    "    \"\"\"\n",
    "    Reads all JSON files in a directory, calculates the average score for each\n",
    "    metric, and calculates the overall average accuracy. It treats 'NaN'\n",
    "    values as 0.0.\n",
    "\n",
    "    Args:\n",
    "        directory_path: The path to the directory containing the JSON files.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - A dictionary with the calculated average scores for each category.\n",
    "        - A float representing the overall average accuracy.\n",
    "    \"\"\"\n",
    "    all_scores = defaultdict(lambda: defaultdict(list))\n",
    "    # New list to store all accuracy scores for the final overall average\n",
    "    all_accuracy_scores = []\n",
    "\n",
    "    data_dir = Path(directory_path)\n",
    "\n",
    "    if not data_dir.is_dir():\n",
    "        print(f\"Error: Directory not found at '{directory_path}'\")\n",
    "        return {}, 0.0\n",
    "\n",
    "    json_files = list(data_dir.glob('*.json'))\n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in '{directory_path}'\")\n",
    "        return {}, 0.0\n",
    "        \n",
    "    print(f\"Found {len(json_files)} JSON files to process...\")\n",
    "\n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                for category, metrics in data.items():\n",
    "                    for metric, value in metrics.items():\n",
    "                        \n",
    "                        # Handle NaN values by converting them to 0.0\n",
    "                        if isinstance(value, float) and math.isnan(value):\n",
    "                            value = 0.0\n",
    "                        \n",
    "                        # Ensure the value is a number before processing\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            all_scores[category][metric].append(value)\n",
    "                            \n",
    "                            # --- NEW ---\n",
    "                            # If the metric is 'answer_correctness', add it to our overall list\n",
    "                            if metric == 'answer_correctness':\n",
    "                                all_accuracy_scores.append(value)\n",
    "                            # --- END NEW ---\n",
    "                                \n",
    "                        else:\n",
    "                            print(f\"‚ö†Ô∏è  Warning: Skipping non-numeric value '{value}' for '{metric}' in {file_path.name}.\")\n",
    "                            continue\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Could not decode JSON from {file_path.name}. Skipping file.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with {file_path.name}: {e}. Skipping file.\")\n",
    "    \n",
    "    # --- Calculate per-category averages ---\n",
    "    average_scores = defaultdict(dict)\n",
    "    for category, metrics in all_scores.items():\n",
    "        for metric, values in metrics.items():\n",
    "            if values:\n",
    "                average_scores[category][metric] = statistics.mean(values)\n",
    "\n",
    "    # --- NEW: Calculate the single overall average accuracy ---\n",
    "    overall_avg_accuracy = statistics.mean(all_accuracy_scores) if all_accuracy_scores else 0.0\n",
    "    \n",
    "    return average_scores, overall_avg_accuracy\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b14c5102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 JSON files to process...\n",
      "\n",
      "--- Average Scores by Category ---\n",
      "\n",
      "## Complex Reasoning\n",
      "  - Average answer_correctness: 0.5478\n",
      "  - Average rouge_score: 0.2591\n",
      "\n",
      "## Contextual Summarize\n",
      "  - Average answer_correctness: 0.6038\n",
      "  - Average coverage_score: 0.6991\n",
      "\n",
      "## Creative Generation\n",
      "  - Average answer_correctness: 0.6316\n",
      "  - Average coverage_score: 0.2897\n",
      "  - Average faithfulness: 0.7666\n",
      "\n",
      "## Fact Retrieval\n",
      "  - Average answer_correctness: 0.4996\n",
      "  - Average rouge_score: 0.3839\n",
      "\n",
      "----------------------------------\n",
      "--- Overall Performance ---\n",
      "üìà Overall Average Accuracy: 0.5683\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "  # ‚ùóÔ∏è IMPORTANT: Change this path to your directory of JSON files.\n",
    "DIRECTORY_PATH = \"/home/nick/projects/GraphRAG-Benchmark/results/embed\" \n",
    "\n",
    "# Calculate the average scores\n",
    "category_averages, overall_accuracy = calculate_scores(DIRECTORY_PATH)\n",
    "# --- END MODIFIED ---\n",
    "\n",
    "# Print the per-category results\n",
    "if category_averages:\n",
    "    print(\"\\n--- Average Scores by Category ---\")\n",
    "    for category, metrics in sorted(category_averages.items()):\n",
    "        print(f\"\\n## {category}\")\n",
    "        for metric, avg_value in sorted(metrics.items()):\n",
    "            print(f\"  - Average {metric}: {avg_value:.4f}\")\n",
    "    \n",
    "    # --- NEW: Print the final overall average ---\n",
    "    print(\"\\n----------------------------------\")\n",
    "    print(\"--- Overall Performance ---\")\n",
    "    print(f\"üìà Overall Average Accuracy: {overall_accuracy:.4f}\")\n",
    "    print(\"---------------------------\")\n",
    "    # --- END NEW ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphBench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
